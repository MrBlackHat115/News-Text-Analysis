{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c23ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import core libraries for data handling and NLP\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "# Import tools for tokenization and stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f4f97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the BBC news dataset from a CSV file\n",
    "bbc_data = pd.read_csv(\"bbc_news.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "bbc_data.head()\n",
    "\n",
    "# Display dataset structure and data types\n",
    "bbc_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd0fc6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the title column and store it in a new DataFrame\n",
    "titles = pd.DataFrame(bbc_data[\"title\"], columns=[\"title\"])\n",
    "\n",
    "# Preview the extracted titles\n",
    "titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca02d95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Convert all titles to lowercase\n",
    "# This ensures consistency (e.g., \"War\" and \"war\" are treated the same)\n",
    "# ---------------------------------------------\n",
    "titles[\"titles_lowercase\"] = titles[\"title\"].str.lower()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load English stopwords from NLTK\n",
    "# ---------------------------------------------\n",
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Remove stopwords from the lowercase titles\n",
    "# - Split each title into words\n",
    "# - Keep only words NOT in the stopword list\n",
    "# - Join them back into a single string\n",
    "# ---------------------------------------------\n",
    "titles[\"review_no_stopwords\"] = titles[\"titles_lowercase\"].apply(\n",
    "    lambda x: \" \".join(\n",
    "        word for word in x.split() if word not in eng_stopwords\n",
    "    )\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Remove punctuation using regular expressions\n",
    "# [^\\w\\s] matches anything that is NOT a word character or whitespace\n",
    "# ---------------------------------------------\n",
    "import re\n",
    "titles[\"review_no_stopwords_no_punct\"] = titles[\"review_no_stopwords\"].apply(\n",
    "    lambda x: re.sub(r\"[^\\w\\s]\", \"\", x)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Tokenize the cleaned text\n",
    "# This converts each title into a list of words\n",
    "# ---------------------------------------------\n",
    "from nltk.tokenize import word_tokenize\n",
    "titles[\"tokenized\"] = titles[\"review_no_stopwords_no_punct\"].apply(\n",
    "    lambda x: word_tokenize(x)\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Initialize the Porter Stemmer\n",
    "# ---------------------------------------------\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Apply stemming to each token in every title\n",
    "# ---------------------------------------------\n",
    "titles[\"stemmed\"] = titles[\"tokenized\"].apply(\n",
    "    lambda tokens: [ps.stem(word) for word in tokens]\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Download WordNet data (required for lemmatization)\n",
    "# ---------------------------------------------\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Initialize the WordNet Lemmatizer\n",
    "# ---------------------------------------------\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Apply lemmatization to the stemmed tokens\n",
    "# Note: Lemmatization is applied word by word\n",
    "# ---------------------------------------------\n",
    "titles[\"lemmatized\"] = titles[\"stemmed\"].apply(\n",
    "    lambda tokens: [lemmatizer.lemmatize(word) for word in tokens]\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# (FIXED) No need to tokenize again\n",
    "# The data is already a list of tokens\n",
    "# ---------------------------------------------\n",
    "titles[\"lemmatized_clean\"] = titles[\"lemmatized\"]\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Combine all raw tokens into one list\n",
    "# Start with an empty list so lists concatenate correctly\n",
    "# ---------------------------------------------\n",
    "tokens_raw_list = sum(titles[\"tokenized\"], [])\n",
    "print(\"First 10 raw tokens:\")\n",
    "print(tokens_raw_list[:10])\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Combine all cleaned + lemmatized tokens into one list\n",
    "# ---------------------------------------------\n",
    "tokens_clean_list = sum(titles[\"lemmatized_clean\"], [])\n",
    "print(\"\\nFirst 10 cleaned & lemmatized tokens:\")\n",
    "print(tokens_clean_list[:10])\n",
    " \n",
    "# ---------------------------------------------\n",
    "# View the processed DataFrame\n",
    "# ---------------------------------------------\n",
    "titles.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf322c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Flatten all lemmatized tokens into one list\n",
    "# This combines tokens from every title\n",
    "# ---------------------------------------------\n",
    "lemmatized_clean = sum(titles[\"lemmatized_clean\"], [])\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate unigrams (n = 1)\n",
    "# ---------------------------------------------\n",
    "unigrams = list(ngrams(lemmatized_clean, 1))\n",
    "unigram_series = pd.Series(unigrams).value_counts()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Generate bigrams (n = 2)\n",
    "# (FIXED: previously n was incorrectly set to 1)\n",
    "# ---------------------------------------------\n",
    "bigrams = list(ngrams(lemmatized_clean, 2))\n",
    "bigram_series = pd.Series(bigrams).value_counts()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Display the results\n",
    "# ---------------------------------------------\n",
    "print(\"Top Unigrams:\")\n",
    "print(unigram_series.head(10))\n",
    "\n",
    "print(\"\\nTop Bigrams:\")\n",
    "print(bigram_series.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed646379",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Convert token list into a single string\n",
    "text_for_pos = \" \".join(tokens_raw_list)\n",
    "\n",
    "# Create a spaCy document\n",
    "doc = nlp(text_for_pos)\n",
    "\n",
    "# Create an empty DataFrame to store tokens and POS tags\n",
    "pos_df = pd.DataFrame(columns=[\"token\", \"pos_tag\"])\n",
    "\n",
    "# Loop through each token in the document\n",
    "for token in doc:\n",
    "    # Create a one-row DataFrame for the current token\n",
    "    row = pd.DataFrame.from_records(\n",
    "        [{\"token\": token.text, \"pos_tag\": token.pos_}]\n",
    "    )\n",
    "    # Append the row to the main DataFrame\n",
    "    pos_df = pd.concat([pos_df, row], ignore_index=True)\n",
    "\n",
    "# Display the first 15 tokens and their POS tags\n",
    "pos_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a478e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Count how often each token-POS pair appears\n",
    "# --------------------------------\n",
    "pos_df_counts = (\n",
    "    pos_df\n",
    "    .groupby([\"token\", \"pos_tag\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"counts\")\n",
    "    .sort_values(\"counts\", ascending=False)\n",
    ")\n",
    "\n",
    "# Show the 10 most common token + POS combinations\n",
    "pos_df_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a684625",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Extract the top 10 most frequent nouns\n",
    "# --------------------------------\n",
    "top_nouns = pos_df_counts[pos_df_counts[\"pos_tag\"] == \"NOUN\"].head(10)\n",
    "\n",
    "# Print the top nouns\n",
    "print(top_nouns)\n",
    "\n",
    "# --------------------------------\n",
    "# Extract the top 10 most frequent verbs\n",
    "# --------------------------------\n",
    "top_verbs = pos_df_counts[pos_df_counts[\"pos_tag\"] == \"VERB\"].head(10)\n",
    "\n",
    "# Print the top nouns\n",
    "print(top_verbs)\n",
    "\n",
    "# --------------------------------\n",
    "# Extract the top 10 most frequent Adjectives\n",
    "# --------------------------------\n",
    "top_adj = pos_df_counts[pos_df_counts[\"pos_tag\"] == \"ADJ\"].head(10)\n",
    "\n",
    "# Print the top nouns\n",
    "print(top_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089623e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty DataFrame for named entities\n",
    "ner_df = pd.DataFrame(columns=[\"token\", \"ner_tag\"])\n",
    "\n",
    "# Extract named entities from the spaCy document\n",
    "for ent in doc.ents:\n",
    "     if pd.isna(ent.label_) is False:\n",
    "        row = pd.DataFrame.from_records(\n",
    "            [{\"token\": ent.text, \"ner_tag\": ent.label_}]\n",
    "        )\n",
    "        ner_df = pd.concat([ner_df, row], ignore_index=True)\n",
    "\n",
    "ner_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c538dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Count frequency of named entities\n",
    "ner_counts = (\n",
    "    ner_df\n",
    "    .groupby([\"token\", \"ner_tag\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"counts\")\n",
    "    .sort_values(\"counts\", ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "# Display the most common named entities\n",
    "ner_counts.head(10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
